{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02eb3fce",
   "metadata": {},
   "source": [
    "# 1. Cargar el set de datos en un dataframe de pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65b1e3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espacio para realizar la importación de librerías\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9a8c8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ubicacionArchivo = \"diabetes.csv\"\n",
    "# Carga de un CSV a un archivo tipo DataFrame por medio de pandas.\n",
    "df = pd.read_csv(ubicacionArchivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3fa269",
   "metadata": {},
   "source": [
    "# 2. Mostrar los mejores resultados para ambos problemas anteriores (regresión y clasificación)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5423dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de X_train: (614, 7)\n",
      "Forma de X_test: (154, 7)\n",
      "Forma de y_train: (614,)\n",
      "Forma de y_test: (154,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Definimos las variables X e y\n",
    "X = df.drop(['Outcome', 'Age'], axis=1)  \n",
    "y = df['Age']  \n",
    "\n",
    "# Dividimos el dataset en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2033)\n",
    "\n",
    "# Imprimimos las formas de los conjuntos de entrenamiento y prueba\n",
    "print(\"Forma de X_train:\", X_train.shape)\n",
    "print(\"Forma de X_test:\", X_test.shape)\n",
    "print(\"Forma de y_train:\", y_train.shape)\n",
    "print(\"Forma de y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "555f0a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error cuadrático medio (MSE): 90.695176249044\n",
      "Mejores hiperparámetros: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "#Para la etiquta \"Age\"\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Definimos los parámetros \n",
    "param_grid = {\n",
    "    'max_depth': [3, 10, 7],\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Instanciamos el modelo de árbol de decisión\n",
    "regression_tree = DecisionTreeRegressor(random_state=2033)\n",
    "\n",
    "# Realizamos la búsqueda en cuadrícula con validación cruzada\n",
    "grid_search = GridSearchCV(regression_tree, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtenemos los mejores parámetros\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Entrenamos el modelo con los mejores parámetros\n",
    "best_regressor = DecisionTreeRegressor(**best_params, random_state=2033)\n",
    "best_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Hacemos predicciones con el conjunto de pruebas\n",
    "y_pred = best_regressor.predict(X_test)\n",
    "\n",
    "# Calculamos el error cuadrático medio (MSE) con los mejores parámetros\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Error cuadrático medio (MSE):\", mse)\n",
    "\n",
    "# Imprimimos los mejores parámetros encontrados\n",
    "print(\"Mejores hiperparámetros:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b117e220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sams\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros: {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 11, 'min_samples_split': 7}\n",
      "Precisión (Accuracy): 0.08441558441558442\n"
     ]
    }
   ],
   "source": [
    "# Para la etiqueta \"Outcome\"\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Definimos los hiperparámetros y sus rangos\n",
    "param_dist = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': randint(1, 30),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20)\n",
    "}\n",
    "\n",
    "# Instanciamos el árbol de decisión para clasificación\n",
    "classifier = DecisionTreeClassifier(random_state=2033)\n",
    "\n",
    "# Realizamos la búsqueda aleatoria para encontrar los mejores hiperparámetros\n",
    "random_search = RandomizedSearchCV(classifier, param_distributions=param_dist, n_iter=100, cv=5, random_state=2033)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtenemos los mejores hiperparámetros\n",
    "best_params = random_search.best_params_\n",
    "print(\"Mejores hiperparámetros:\", best_params)\n",
    "\n",
    "# Obtenemos el mejor modelo\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Hacemos predicciones con el conjunto de pruebas\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculamos la precisión (accuracy) del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisión (Accuracy):\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369659f5",
   "metadata": {},
   "source": [
    "# 3. Importe las librerías para el VotingClassifier y el RandomForest (tanto para regresión como para clasificación)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11b44b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para el VotingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Para el RandomForest de regresión\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Para el RandomForest de clasificación\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9075bbdb",
   "metadata": {},
   "source": [
    "# 4. Entrene un modelo de votación para regresión con 3 modelos de regresión de su preferencia (por ejemplo: regresión lineal, árbol de decisión, KNeighborRegressor), utilice las mismas muestras de “X” y “y” que en la actividad anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4229e5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingRegressor(estimators=[(&#x27;linear&#x27;, LinearRegression()),\n",
       "                            (&#x27;tree&#x27;, DecisionTreeRegressor()),\n",
       "                            (&#x27;knn&#x27;, KNeighborsRegressor())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingRegressor</label><div class=\"sk-toggleable__content\"><pre>VotingRegressor(estimators=[(&#x27;linear&#x27;, LinearRegression()),\n",
       "                            (&#x27;tree&#x27;, DecisionTreeRegressor()),\n",
       "                            (&#x27;knn&#x27;, KNeighborsRegressor())])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>linear</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>tree</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>knn</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsRegressor</label><div class=\"sk-toggleable__content\"><pre>KNeighborsRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingRegressor(estimators=[('linear', LinearRegression()),\n",
       "                            ('tree', DecisionTreeRegressor()),\n",
       "                            ('knn', KNeighborsRegressor())])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "# Definir los estimadores\n",
    "estimators = [('linear', LinearRegression()), ('tree', DecisionTreeRegressor()), ('knn', KNeighborsRegressor())]\n",
    "\n",
    "# Instanciar el modelo de votación para regresión\n",
    "voting_regressor = VotingRegressor(estimators)\n",
    "\n",
    "# Entrenar el modelo de votación\n",
    "voting_regressor.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b166a50c",
   "metadata": {},
   "source": [
    "# 5. Pruebe el modelo de votación con la muestra de pruebas y evalúe con MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f0062bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error cuadrático medio (MSE) del modelo de votación: 88.14423538206117\n"
     ]
    }
   ],
   "source": [
    "# Hacer predicciones con el conjunto de pruebas\n",
    "y_pred_voting = voting_regressor.predict(X_test)\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE) del modelo de votación\n",
    "mse_voting = mean_squared_error(y_test, y_pred_voting)\n",
    "print(\"Error cuadrático medio (MSE) del modelo de votación:\", mse_voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca8d18",
   "metadata": {},
   "source": [
    "# 6. ¿Tuvo mejores o peores resultados?, ¿qué tanto tardó en entrenarse?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812339b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e152276e",
   "metadata": {},
   "source": [
    "# 7. Entrene un modelo de votación para clasificación con 3 modelos de clasificación de su preferencia (por ejemplo: regresión logística, árbol de decisión y KNeighborClassifier), utilice las mismas muestras de “X” y “y” que la actividad anterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "750895a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión (Accuracy) del modelo de votación: 0.08441558441558442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sams\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Supongamos que X, y representan tus datos y objetivos de clasificación respectivamente\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2033)\n",
    "\n",
    "# Definir los estimadores\n",
    "estimators = [('logistic', LogisticRegression()), ('tree', DecisionTreeClassifier()), ('knn', KNeighborsClassifier())]\n",
    "\n",
    "# Instanciar el modelo de votación para clasificación\n",
    "voting_classifier = VotingClassifier(estimators)\n",
    "\n",
    "# Entrenar el modelo de votación\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones con el conjunto de pruebas\n",
    "y_pred = voting_classifier.predict(X_test)\n",
    "\n",
    "# Calcular la precisión (accuracy) del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisión (Accuracy) del modelo de votación:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dee33e",
   "metadata": {},
   "source": [
    "# 8. Pruebe el modelo de votación con la muestra de pruebas y evalúe con Accuracy_Score y F1_Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e5d04de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión (Accuracy) del modelo de votación: 0.08441558441558442\n",
      "Puntuación F1 (micro) del modelo de votación: 0.08441558441558442\n",
      "Puntuación F1 (macro) del modelo de votación: 0.01229129550506882\n",
      "Puntuación F1 (weighted) del modelo de votación: 0.041393060720791824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Supongamos que X_test y y_test son tus conjuntos de prueba\n",
    "\n",
    "# Hacer predicciones con el conjunto de pruebas\n",
    "y_pred_voting = voting_classifier.predict(X_test)\n",
    "\n",
    "# Calcular la precisión (accuracy) del modelo\n",
    "accuracy_voting = accuracy_score(y_test, y_pred_voting)\n",
    "print(\"Precisión (Accuracy) del modelo de votación:\", accuracy_voting)\n",
    "\n",
    "# Calcular la puntuación F1 del modelo con average='micro'\n",
    "f1_voting_micro = f1_score(y_test, y_pred_voting, average='micro')\n",
    "print(\"Puntuación F1 (micro) del modelo de votación:\", f1_voting_micro)\n",
    "\n",
    "# Calcular la puntuación F1 del modelo con average='macro'\n",
    "f1_voting_macro = f1_score(y_test, y_pred_voting, average='macro')\n",
    "print(\"Puntuación F1 (macro) del modelo de votación:\", f1_voting_macro)\n",
    "\n",
    "# Calcular la puntuación F1 del modelo con average='weighted'\n",
    "f1_voting_weighted = f1_score(y_test, y_pred_voting, average='weighted')\n",
    "print(\"Puntuación F1 (weighted) del modelo de votación:\", f1_voting_weighted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de684af",
   "metadata": {},
   "source": [
    "# 9. ¿Tuvo mejores o peores resultados?, ¿qué tanto tardó en entrenarse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a264e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4eba8aa",
   "metadata": {},
   "source": [
    "# 10. Entrene un modelo de bosque aleatorio para clasificación, puede emplear los hiperparámetros que considere adecuados (de esto dependerá la eficiencia del modelo), para el entrenamiento se debe usar la misma muestra que en los casos anteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53d08866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión (Accuracy) del modelo de Bosque Aleatorio: 0.07792207792207792\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2033)\n",
    "\n",
    "# Definir y entrenar el modelo de Bosque Aleatorio para clasificación\n",
    "random_forest_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=2033)\n",
    "random_forest_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones con el conjunto de pruebas\n",
    "y_pred = random_forest_classifier.predict(X_test)\n",
    "\n",
    "# Calcular la precisión (accuracy) del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisión (Accuracy) del modelo de Bosque Aleatorio:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d5521d",
   "metadata": {},
   "source": [
    "# 11. Se debe evaluar el modelo con F1_Score y Accuracy_Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40983dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión (Accuracy) del modelo de Bosque Aleatorio: 0.07792207792207792\n",
      "Puntuación F1 (micro) del modelo de Bosque Aleatorio: 0.07792207792207792\n",
      "Puntuación F1 (macro) del modelo de Bosque Aleatorio: 0.03254388677321008\n",
      "Puntuación F1 (weighted) del modelo de Bosque Aleatorio: 0.07422544702961655\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hacer predicciones con el conjunto de pruebas\n",
    "y_pred_rf = random_forest_classifier.predict(X_test)\n",
    "\n",
    "# Calcular la precisión (accuracy) del modelo\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Precisión (Accuracy) del modelo de Bosque Aleatorio:\", accuracy_rf)\n",
    "\n",
    "# Calcular la puntuación F1 del modelo con average='micro'\n",
    "f1_rf_micro = f1_score(y_test, y_pred_rf, average='micro')\n",
    "print(\"Puntuación F1 (micro) del modelo de Bosque Aleatorio:\", f1_rf_micro)\n",
    "\n",
    "# Calcular la puntuación F1 del modelo con average='macro'\n",
    "f1_rf_macro = f1_score(y_test, y_pred_rf, average='macro')\n",
    "print(\"Puntuación F1 (macro) del modelo de Bosque Aleatorio:\", f1_rf_macro)\n",
    "\n",
    "# Calcular la puntuación F1 del modelo con average='weighted'\n",
    "f1_rf_weighted = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "print(\"Puntuación F1 (weighted) del modelo de Bosque Aleatorio:\", f1_rf_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8955e5f2",
   "metadata": {},
   "source": [
    "# 12. ¿Qué tal se comportó?, ¿mejoró o empeoró con respecto a los modelos anteriores?,  ¿tomó más o menos tiempo para entrenarse?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c44187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80a19080",
   "metadata": {},
   "source": [
    "# 13. Entrene nuevamente el bosque aleatorio, pero cambiando los hiperparámetros de manera radical. ¿Qué tal se comportó el modelo?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "378e4374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión (Accuracy) del modelo de Bosque Aleatorio (nuevos hiperparámetros): 0.09090909090909091\n",
      "Puntuación F1 (weighted) del modelo de Bosque Aleatorio (nuevos hiperparámetros): 0.057527143088640424\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Supongamos que X, y representan tus datos y objetivos de clasificación respectivamente\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2033)\n",
    "\n",
    "# Definir y entrenar el modelo de Bosque Aleatorio con hiperparámetros diferentes\n",
    "random_forest_classifier_new = RandomForestClassifier(n_estimators=50, max_depth=5, min_samples_split=5, random_state=2033)\n",
    "random_forest_classifier_new.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones con el conjunto de pruebas\n",
    "y_pred_new = random_forest_classifier_new.predict(X_test)\n",
    "\n",
    "# Calcular la precisión (accuracy) del modelo\n",
    "accuracy_new = accuracy_score(y_test, y_pred_new)\n",
    "print(\"Precisión (Accuracy) del modelo de Bosque Aleatorio (nuevos hiperparámetros):\", accuracy_new)\n",
    "\n",
    "# Calcular la puntuación F1 del modelo con average='weighted'\n",
    "f1_new = f1_score(y_test, y_pred_new, average='weighted')\n",
    "print(\"Puntuación F1 (weighted) del modelo de Bosque Aleatorio (nuevos hiperparámetros):\", f1_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ac20ad",
   "metadata": {},
   "source": [
    "# 14. Escriba sus conclusiones obtenidas al utilizar modelos de ensamble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651d92ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
